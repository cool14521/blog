
---
layout: post
title: "ML 课程笔记"
modified: 2015-08-09 18:35:58 +0800
tags: [机器学习, Coursera]

---

第一次学Coursera 上的[ML 课程](https://www.coursera.org/learn/machine-learning/)的时候，遗漏了很多，而且那会儿笔记也做得不够，基础又差。
最近重温，整理一些重要的笔记吧。

## Linear Regression

最简单也最常用的就是线性回归，最终就是为了学习如下的 \`theta\` :

\`f(x) = theta^T vec x\`

而怎么来学习这个f 函数及其\`theta \`？衡量标准是什么？最简单的，就让f 跟目标y 的差越小越好，也就是SSE(sum of square eror)这个目标函数（或叫损失函数）:

\` J(f) = 1/(2m) sum_(i=1)^m(f_i-y_i)^2 \`

其中m 为样本的个数。

### Gradient descent
而求最佳\`theta\` 的问题也就转化成求这个凸函数的最小值了。而要让J(f) 变小，其实就是让它的梯度渐渐接近0，那我们就可以以如后方式迭代更新：

\`
theta_j = theta_j - alpha del/(del theta_j) J(f) = theta_j - alpha 1/m sum_(i=1)^m (f_i - y_i) x\_(ij)
\`

其中 \`alpha \` 为learning rate，而\`del/(del theta_j) f= vec x_j\`

### Feature scaling
比较建议就是把特征归一一下，最简单的归一就是减去mean 再除以range，这样就归一到[-1,1] 了。

### Normal equation
其实如果数据量小，我们可以直接求解参数的。因为

\` y = X vec theta \`

其中，X 为样本矩阵，每一行为一个样本。
则求解后 \`theta = (X^T X)^(-1) X^T y\`



## Logistic Regression

用于二分类的LR 其实就只是在线性回归的基础上加了一个sigmoid 函数，线性回归的正负数结果，也就对应了LR 的0、1 两个分类。

\`
f(x) = sigmoid(theta^T vec x) = g(theta^T vec x) = 1/(1+e^( - theta^T vec x))
\`

因为y只可能为0 或1 ，而其损失函数则构造为：

\`
J\_(LR)(f) = 1/(2m) sum_(i=1)^m - y_i log(f_i ) - (1-y_i ) log (1-f_i )
\`

其实就是为了表征，y-f 越接近，损失函数的值就应该越小。

加上L2 的Regulization Term 则为：

\`
J\_(LR-RT)(f) = 1/(2m) sum_(i=1)^m - y_i log(f_i ) - (1-y_i ) log (1-f_i ) + lamda/(2m) sum\_(j=1)^n theta_j^2
\`

其中n 为 \`theta\` 的长度，\`theta(0)\` 是常数项，不纳入RT[^RT] 中的。

我们也可以用同样的梯度下降方式迭代更新\`theta \`:

\`
theta_j = theta_j - alpha del/(del theta_j) J(f) = theta_j - alpha 1/m sum_(i=1)^m (f_i - y_i) x\_(ij)
\`

而其中看似跟线性回归一样的梯度，实际上是通过推导损失函数的偏导数化简得来的。

### Optimisation algorithms

- Gradient descent
- Conjugate gradient
- BFGS
- L-BFGS

## Neural Networks

逻辑回归中 \`f(x) = g(theta^T vec x)\` 其实只需要学一个 \`theta \`。如果我们学多个比如L 个\`Theta \` 会是怎样呢？ \`Theta \` 每一行为原来的一个\`theta_i^T\`，这样才能保证计算中间的\`f_i\` 仍然为向量。
\`f_1(x) = g(Theta^1 vec x)\`

\`
i= 2:L;
f_i = g(Theta^i f_(i-1));
end
\`

最后得到的\`f_L\` 即为最终结果。
但是NN[^NN] 中，中间函数不叫f 而叫做 a (activation function)，且有\`a^1 = vec x\`; \`a^(i+1) = f_(i)\`; \`theta_i^T(0) -= 1\`(因为每一次的\`theta \` 我们都需要增加常数项)

NN 中，上标\`i in [1,L]\` 就代表对应层数的\`Theta\` 和 \`a\`，第一层和最后一层分别叫做输入输出层，中间的都叫做hiden layers 。

而因为\`a^(i+1) = g(Theta^i a^i )\` 则 \`a^(i+1)\` 的行数 跟\`Theta^i\` 行数一致，列数就正好跟  \`a^(i)\` 的列数一致

### cost function

假设单次\`a^i\` 的 cost 为LR 的cost function \`J\_(LR)(a^i)\` (设其中不包含RT)

则二分类NN 的整体cost function 即为：

\`
J(a) = J\_(LR)(a^L) + lamda/2 * L2RT
\`

而L2RT 即为所有\`Theta\` 的L2 RT，即非常数项的平方和再除以样本个数m。

而如果NN 输出结果为多分类，即\`f_L\` 不为单值0/1，而是One-hot vector（即其中某一位为1其余位为0 代表确切的某个分类）。则\`f_L\` 的长度K 即为分类的数量。

那么多分类的cost function 则为

\`
J(a) = sum_(i=1)^K J\_(LR)(a_i^L) + lamda/2 *  L2RT
\`


### gradient computation

同样，我们需要求出 \`del/(del Theta_(ij)^l) J(a) \` 来最小化目标函数。
之前针对LR \`del/(del theta_j) J(f) = (f-y) del/(del theta_j) f = Delta y del/(del theta_j) f\`

则针对NN 则有一般形式：
\`del/(del Theta^i) J(a^(i+1)) = Delta a^(i+1) del/(del Theta^i) g(Theta^i a^i)\`

反向传播假设\`a^i = g((Theta^i)^T a^(i+1))\`，则由 \`Delta y = Delta x del/(del x) y\`归并并推导得[^ANN]：
 \`Delta a^i = (Theta^i)^T del/(del Theta_i) J(a^i) = (Theta^i)^T Delta a^(i+1) del/(del Theta^i) g(Theta^i a^i)= (Theta^i)^T Delta a^(i+1) dot xx  (a^i dotxx (1-a^i)) \`
 
 其中\`dot xx\` 为点乘

[^NN]: Neural Networks
[^RT]: Regularisation Term
[^ANN]: 参照此书的“人工神经网络” 章节 [《机器学习》](http://book.douban.com/subject/1102235/) 本博推导不一定正确

**Mathjax was not loaded successfully**{:.mathjax_alt} 
{% comment %}
<script type='text/x-mathjax-config'> MathJax.Hub.Config({ asciimath2jax: { delimiters: [['`','`'],['$', '$']] }}); </script>
<script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML' async='async'></script>
{% endcomment %}

