
---
layout: post
title: "ML 课程笔记"
modified: 2015-08-09 18:35:58 +0800
tags: [机器学习, Coursera]

---

第一次学Coursera 上的[ML 课程](https://www.coursera.org/learn/machine-learning/)的时候，遗漏了很多，而且那会儿笔记也做得不够，基础又差。
最近重温，整理一些重要的笔记吧。

## Linear Regression

最简单也最常用的就是线性回归，最终就是为了学习如下的 \`theta\` :

\`f(x) = theta^T vec x\`

而怎么来学习这个f 函数及其\`theta \`？衡量标准是什么？最简单的，就让f 跟目标y 的差越小越好，也就是SSE(sum of square eror)这个目标函数（或叫损失函数）:

\` J(f) = 1/(2m) sum_(i=1)^m(f_i-y_i)^2 \`

其中m 为样本的个数。

### Gradient descent
而求最佳\`theta\` 的问题也就转化成求这个凸函数的最小值了。而要让J(f) 变小，其实就是让它的梯度渐渐接近0，那我们就可以以如后方式迭代更新：

\`
theta_j = theta_j - alpha del/(del theta_j) J(f) = theta_j - alpha 1/m sum_(i=1)^m (f_i - y_i) x_i
\`

其中 \`alpha \` 为learning rate

### Feature scaling
比较建议就是把特征归一一下，最简单的归一就是减去mean 再除以range，这样就归一到[-1,1] 了。

### Normal equation
其实如果数据量小，我们可以直接求解参数的。因为

\` y = X vec theta \`

其中，X 为样本矩阵，每一行为一个样本。
则求解后 \`theta = (X^T X)^(-1) X^T y\`



## Logistic Regression

用于二分类的LR 其实就只是在线性回归的基础上加了一个sigmoid 函数，线性回归的正负数结果，也就对应了LR 的0、1 两个分类。

\`
f(x) = sigmoid(theta^T vec x)  = 1/(1+e^( - theta^T vec x))
\`

因为y只可能为0 或1 ，而其损失函数则构造为：

\`
J(f) = 1/(2m) sum_(i=1)^m - y_i log(f_i ) - (1-y_i ) log (1-f_i )
\`

其实就是为了表征，y-f 越接近，损失函数的值就应该越小。

我们也可以用同样的梯度下降方式迭代更新\`theta \`:

\`
theta_j = theta_j - alpha del/(del theta_j) J(f) = theta_j - alpha 1/m sum_(i=1)^m (f_i - y_i) x_i
\`

而其中看似跟线性回归一样的梯度，实际上是通过推倒损失函数的偏导数化简得来的。

### Optimisation algorithms

- Gradient descent
- Conjugate gradient
- BFGS
- L-BFGS

**Mathjax was not loaded successfully**{:.mathjax_alt} 
{% comment %}
<script type='text/x-mathjax-config'> MathJax.Hub.Config({ asciimath2jax: { delimiters: [['`','`'],['$', '$']] }}); </script>
<script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML' async='async'></script>
{% endcomment %}

