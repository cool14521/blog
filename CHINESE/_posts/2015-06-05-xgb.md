---
layout: post
title: "GBT and XGB"
modified: 2015-06-05 17:20:36 +0800
tags: [GBDT, XGB]
image:
  feature: 
  credit: 
  creditlink: 
comments: 
share: 

---

## GBT 目标函数

GBT 其实就是多个若分类器的集合：

\`hat y_i^k = sum_(j=1)^k f_j(x_i) = hat y\_i^(k-1) + f_k(x_i), quad f_j in mathcal F \`

其中$\mathcal{F}$ 作为一个函数空间，为学到的所有函数的集合。而k 为迭代次数，也就是树的个数。

则第k 次迭代后目标函数值：
\`Obj^k = sum l(y_i, hat y_i^k) +sum_(j=1)^k Omega(f_j) = sum l(y_i, hat y_i^(k-1) + f_k(x_i)) + Omega(f_k) + C \`

$l$ 为loss function 比如logistic loss
:$l(y_i,\hat y_i) = y_i \ln(1 + e^{-\hat y_i}) + (1-y_i) \ln(1+e^{\hat y_i})$

而\` sum_(j=1)^k Omega(f_j) \` 和 $\Omega(f_k)$ 为Regularization Term，用于表示树的复杂度（比如深度、节点数、L2）

C 为常数，代表k 轮之前的树的复杂度之和；因为k 轮迭代之前的Regulation Term，在第k 轮迭代时为常数了。
第k 轮时，就 $f_k$ 是一个变量，也就是需要来学习并最小化目标函数的。

此目标函数，复杂度就依赖于loss function 的复杂度。

一般使用梯度下降方式就是将loss function 的梯度（也就是一阶导数绝对值 \`|l^'(y_i, hat y_i^k)|\`） 趋近于0 的方向行进，从而得到极值。

其他GBT 知识可参见：

- [GBDT 原理][GBDT]
- [GBDT/MART 概念简介][GBDT Intro]
- [随机森林与GBDT]

## XGB Loss Function

而XGB 中，如果我们使用[泰勒展开]，有

\`
l(y+ Delta y) = sum_(n=0)^( oo ) (l^((n))(y))/(i!) (Delta y)^n ~= l(y) + l^'(y) Delta y + l^('') (y) Delta y ^2 
\`

在第*k-1*轮迭代后，设
\`
Delta y = f_k(x_i)
\`
，而针对 $l(y)$，一阶导数、二阶导数分别为：

\`g_i = l\_(hat y_i^(k-1))^'(y_i,hat y_i^(k-1))\`

\`h_i = l\_(hat y_i^(k-1))^('')(y_i,hat y_i^(k-1))\`

那么第k 轮时候的迭代目标函数为：

\`
Obj^k ~= sum (l(y_i,hat y_i^(k-1))+ g_i f_k(x_i) + 1/2 h_i f_k(x_i)^2) + Omega(f_k) + C
\`

那么目标函数就只依赖loss function 的一阶导数、二阶导数（复杂度相形于除square loss 外的loss function 更低）。
而在第k 轮时，去掉优化时的常数项，目标函数可以改写为：
\`Obj^k ~= sum (g_i f_k(x_i) + 1/2 h_i f_k(x_i)^2) + Omega(f_k)\`

## XGB Regularization Term
此项用来限制树的复杂度，比如使用叶子数量V 和 L2 进行定义：

\`
Omega(f_k) = gamma V + 1/2 lamda sum_(j=1)^V(w_j^2)
\`

其中$w_j$ 为每个叶子节点的值。

树的学习结果，$\vec w$ 代表所有的叶子节点值的向量，设 q 函数将$x_i$ 映射到叶子节点j $q(x_i)=j$，叶子节点集合设为 \`I_j = {i | q(x_i)=j}\`

则\`f_k(x_i) = w_(q(x_i))(x_i)\`

将RT 及上式代入目标函数：

\`Obj^k ~= sum\_(i=1)^n (g_i w_q(x_i) + 1/2 h_i w_q(x_i)^2) + gamma V + 1/2 lamda sum\_(j=1)^V(w_j^2)\`
\`\  \  = sum\_(j=1)^T [w_j sum\_(i in I_j) g_i + 1/2 w_j^2 ( lamda +sum_(i in I_j) h_i) ] + gamma V\`


再设定：
$$G_j = \sum\_{i \in I_j} g_i \quad H_j = \sum_{i \in I_j} h_i$$
其实他们就分别代表：划分到某个叶子节点j 上的节点，对应的一阶、二阶导数值之和。

\`Obj^k ~= sum\_(j=1)^T [w_j G_j + 1/2 w_j^2 ( lamda + H_j) ] + gamma V\`

故而，这个关于$w_j$ 的一元二次式，当
\`w_j^** = -b/(2a) = - G_j / (lamda + H_j)\`

\`Obj\_(min) = (4ac-b^2)/(4a) =  gamma V - sum_(j=1)^V G_j^2/(2(lamda + H_j)) \`

## 建树

建立树结构时，就是每每通过找某个叶子节点(V 为1)的分裂点。而增益：
\` "Gain" = Obj^(parent) - Obj^("left") - Obj^(right)  \`
\` \ \  = - gamma - G_p^2/(2(lamda + H_p)) + G_L^2/(2(lamda + H_L))  + G_R^2/(2(lamda + H_R)) \`
\` \ \  = - gamma - (G_L+G_R)^2/(2(lamda + (H_L+G_R))) + G_L^2/(2(lamda + H_L))  + G_R^2/(2(lamda + H_R)) \`

有两种方式：
- Gain 为非正数时即刻停止分裂
- 一直分裂到最底端，然后自底向上剪枝剪去Gain 为非正数枝叶

[泰勒展开]: http://mathworld.wolfram.com/TaylorSeries.html
[XGB]: http://www.52cs.org/?p=429
[GBDT]: http://blog.csdn.net/dark_scope/article/details/24863289
[GBDT Intro]: http://suanfazu.com/t/gbdt-mart-gai-nian-jian-jie/133
[随机森林与GBDT]: http://www.cnblogs.com/leftnoteasy/archive/2011/03/07/random-forest-and-gbdt.html
[GBts in MLlib]: http://blog.jobbole.com/85408/
**Mathjax was not loaded successfully**{:.mathjax_alt} 
<script type='text/x-mathjax-config'> MathJax.Hub.Config({ config: ['TeX-MML-AM_HTMLorMML.js'], tex2jax: { inlineMath: [ ['$', '$'] ] }, asciimath2jax: { delimiters: [ ['`','`']] } }); </script> <script src='http://mathjax.josephjctang.com/MathJax.js' async='async'></script>

